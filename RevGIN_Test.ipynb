{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "r0VJxZnIDpJT",
        "igYJ4y99D-bU",
        "z_dcpDhREijU",
        "fTEnWpOOEZW9",
        "8rh6Ca1XE6ar",
        "RRCYp7yuLxMe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zoro1092000/xxxx/blob/main/RevGIN_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Pip & Import."
      ],
      "metadata": {
        "id": "4KD-KUsKDiLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LzKgPv8elRE",
        "outputId": "035ddaa4-f667-4cd6-f61f-89da23d73c7a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "H7D6x31EU0fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abecd76-4b1f-4833-b61c-58f8f9b9c8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.1.0.post1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: deepdish==0.3.5 in /usr/local/lib/python3.7/dist-packages (0.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deepdish==0.3.5) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepdish==0.3.5) (1.21.6)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from deepdish==0.3.5) (3.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tables->deepdish==0.3.5) (21.3)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables->deepdish==0.3.5) (2.8.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tables->deepdish==0.3.5) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-optimizer in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch-optimizer) (1.12.1+cu113)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from torch-optimizer) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch-optimizer) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install deepdish==0.3.5\n",
        "!pip install torch-optimizer\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "from torch_scatter import scatter_add\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch_optimizer as optimization\n",
        "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout, LayerNorm\n",
        "from torch_geometric.nn import GINConv, GroupAddRev, GATv2Conv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from itertools import chain\n",
        "import pickle\n",
        "import h5py\n",
        "import deepdish as dd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import inspect\n",
        "import time\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def time_since(start):\n",
        "    now = time.time()\n",
        "    s = now - start\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    h = math.floor(m / 60)\n",
        "    m -= h * 60\n",
        "    if h == 0:\n",
        "        if m == 0:\n",
        "            return '%ds' % s\n",
        "        else:\n",
        "            return '%dm %ds' % (m, s)\n",
        "    else:\n",
        "        return '%dh %dm %ds' % (h, m, s)\n"
      ],
      "metadata": {
        "id": "X5j9SrCo4uM5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Data"
      ],
      "metadata": {
        "id": "r0VJxZnIDpJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data utils\n",
        "def h5group_to_dict(h5group):\n",
        "    group_dict = {k: v[()] for k, v in chain(h5group.items(), h5group.attrs.items())}\n",
        "    return group_dict\n",
        "\n",
        "def sub_dict(full_dict, *keys, to_tensor):\n",
        "    return {k: torch.tensor(full_dict[k]) if to_tensor else full_dict[k] for k in keys if k in full_dict}\n",
        "\n",
        "def build_graph_from_dict_pyg(graph_dict, to_tensor=True):\n",
        "    from torch_geometric.data import Data\n",
        "\n",
        "    g = Data(**sub_dict(graph_dict, 'edge_index', 'x', 'y', 'edge_attr', 'edge_y', to_tensor=to_tensor))\n",
        "    return g\n",
        "\n",
        "# Data loader\n",
        "class GraphDataLoader(DataLoader):\n",
        "    def __init__(self, dataset, batch_size=128, shuffle=False, num_workers=0):\n",
        "\n",
        "        def collate_graph(graph_obj_list):\n",
        "            from torch_geometric.data import Batch\n",
        "            batch = Batch.from_data_list(graph_obj_list)\n",
        "            return batch\n",
        "\n",
        "        super().__init__(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=collate_graph,\n",
        "            num_workers=num_workers)\n",
        "\n",
        "# BotnetDataset\n",
        "class BotnetDataset(Dataset):\n",
        "\n",
        "    def __init__(self, name='chord', root='data/botnet', split='train', graph_format='pyg', split_idx=None, add_nfeat_ones=True,\n",
        "                 in_memory=True):\n",
        "        super().__init__()\n",
        "        assert name in ['chord', 'debru', 'kadem', 'leet', 'c2', 'p2p']\n",
        "        assert split in ['train', 'val', 'test']\n",
        "\n",
        "        if isinstance(root, str):\n",
        "            root = osp.expanduser(osp.normpath(root))\n",
        "\n",
        "        self.name = name\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.split_idx = split_idx\n",
        "        self.add_nfeat_ones = add_nfeat_ones\n",
        "\n",
        "        self._graph_format = graph_format\n",
        "        if split == 'train':\n",
        "            self.path = self.processed_paths[0]\n",
        "            self.num_graphs = 768\n",
        "        elif split == 'val':\n",
        "            self.path = self.processed_paths[1]\n",
        "            self.num_graphs = 96\n",
        "        elif split == 'test':\n",
        "            self.path = self.processed_paths[2]\n",
        "            self.num_graphs = 96\n",
        "\n",
        "        if in_memory:\n",
        "            self.data = dd.io.load(self.path)  # dictionary\n",
        "            self.data_type = 'dict'\n",
        "        else:\n",
        "            # self.data = h5py.File(self.path, 'r')\n",
        "            self.data = None    # defer opening file in each process to make multiprocessing work\n",
        "            self.data_type = 'file'\n",
        "            \n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "        return osp.join(self.root, 'processed')\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return [self.name + '_' + s + '.hdf5' for s in ('train', 'val', 'test')]\n",
        "\n",
        "    @property\n",
        "    def processed_paths(self):\n",
        "        return [osp.join(self.processed_dir, f) for f in self.processed_file_names]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_graphs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.data_type == 'dict':\n",
        "            graph_dict = self.data[str(index)]\n",
        "        elif self.data_type == 'file':\n",
        "            if self.data is None:\n",
        "                # only open once in each process\n",
        "                self.data = h5py.File(self.path, 'r')\n",
        "            graph_dict = h5group_to_dict(self.data[str(index)])\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        # graph_format == 'pyg':\n",
        "        return build_graph_from_dict_pyg(graph_dict)\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(self.num_graphs):\n",
        "            yield self[i]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}(topology: {self.name} | split: {self.split} | ' \\\n",
        "               f'#graphs: {len(self)} | graph format: {self.graph_format})'\n"
      ],
      "metadata": {
        "id": "JHHcJ7B-4BmU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III. Measure Performancce"
      ],
      "metadata": {
        "id": "igYJ4y99D-bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Metrics"
      ],
      "metadata": {
        "id": "z_dcpDhREijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1(target, pred, label):\n",
        "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    tp = np.sum((target==label) & (pred==label))\n",
        "    fp = np.sum((target!=label) & (pred==label))\n",
        "    fn = np.sum((pred!=label) & (target==label))\n",
        "    \n",
        "    if tp+fp==0 or tp+fn==0:\n",
        "      return np.nan\n",
        "\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    \n",
        "    if precision+recall==0:\n",
        "      return np.nan\n",
        "      \n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def f1_macro(pred, target):\n",
        "    return np.mean([f1(target, pred, label) for label in range(0, 2)])\n",
        "\n",
        "\n",
        "def accuracy(pred, target):\n",
        "    return (pred == target).sum().item() / len(target)\n",
        "\n",
        "\n",
        "def true_positive(pred, target):\n",
        "    return (target[pred == 1] == 1).sum().item()\n",
        "\n",
        "\n",
        "def false_positive(pred, target):\n",
        "    return (target[pred == 1] == 0).sum().item()\n",
        "\n",
        "\n",
        "def true_negative(pred, target):\n",
        "    return (target[pred == 0] == 0).sum().item()\n",
        "\n",
        "\n",
        "def false_negative(pred, target):\n",
        "    return (target[pred == 0] == 1).sum().item()\n",
        "\n",
        "\n",
        "def recall(pred, target):\n",
        "    try:\n",
        "        return true_positive(pred, target) / (target == 1).sum().item()\n",
        "    except:  # divide by zero\n",
        "        return -1\n",
        "\n",
        "\n",
        "def precision(pred, target):\n",
        "    try:\n",
        "        prec = true_positive(pred, target) / (pred == 1).sum().item()\n",
        "        return prec\n",
        "    except:  # divide by zero\n",
        "        return -1\n",
        "\n",
        "\n",
        "def f1_score(pred, target):\n",
        "    prec = precision(pred, target)\n",
        "    rec = recall(pred, target)\n",
        "    try:\n",
        "        return 2 * (prec * rec) / (prec + rec)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def false_positive_rate(pred, target):\n",
        "    try:\n",
        "        return false_positive(pred, target) / (target == 0).sum().item()\n",
        "    except:  # divide by zero\n",
        "        return -1\n",
        "\n",
        "\n",
        "def false_negative_rate(pred, target):\n",
        "    try:\n",
        "        return false_negative(pred, target) / (target == 1).sum().item()\n",
        "    except:  # divide by zero\n",
        "        return -1\n"
      ],
      "metadata": {
        "id": "BN-QIjMREw-7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Evaluation"
      ],
      "metadata": {
        "id": "fTEnWpOOEZW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_metrics(target, pred_prob, threshold=0.5):\n",
        "    if isinstance(target, torch.Tensor):\n",
        "        target = target.cpu().numpy()\n",
        "    if isinstance(pred_prob, torch.Tensor):\n",
        "        pred_prob = pred_prob.cpu().numpy()\n",
        "\n",
        "    pred = (pred_prob >= threshold).astype(int)\n",
        "\n",
        "    acc = accuracy(pred, target)\n",
        "    fpr = false_positive_rate(pred, target)\n",
        "    fnr = false_negative_rate(pred, target)\n",
        "    rec = recall(pred, target)\n",
        "    prc = precision(pred, target)\n",
        "    f1 = f1_score(pred, target)\n",
        "    f1macro = f1_macro(pred, target)\n",
        "    result_dict = {'acc': acc, 'fpr': fpr, 'fnr': fnr, 'rec': rec, 'prc': prc, 'f1': f1, 'f1_macro': f1macro}\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "\n",
        "def dict_value_add(dict1, dict2):\n",
        "    result = {key: dict1.get(key, 0) + dict2.get(key, 0)\n",
        "              for key in set(dict1) | set(dict2)}\n",
        "    return result\n",
        "\n",
        "\n",
        "def dict_value_div(dict, n):\n",
        "    result = {key: value / n for key, value in dict.items()}\n",
        "    return result\n",
        "\n",
        "\n",
        "def eval_predictor(dataset, predictor):\n",
        "    result_dict_avg = {}\n",
        "    loss_avg = 0\n",
        "\n",
        "    for data in dataset:\n",
        "        # prediction\n",
        "        try:\n",
        "            pred_prob, loss = predictor(data)\n",
        "            loss_avg += loss\n",
        "        except ValueError:  # if \"too many values to unpack\"\n",
        "            pred_prob = predictor(data)\n",
        "\n",
        "        # get the ground truth target\n",
        "        # graph_format == 'pyg':\n",
        "        target = data.y\n",
        "\n",
        "        # compute the evaluation metrics\n",
        "        result_dict = eval_metrics(target, pred_prob)\n",
        "\n",
        "        result_dict_avg = dict_value_add(result_dict_avg, result_dict)\n",
        "\n",
        "    # average the metrics across all graphs in the dataset as final results\n",
        "    result_dict_avg = dict_value_div(result_dict_avg, len(dataset))\n",
        "    loss_avg = loss_avg / len(dataset)\n",
        "\n",
        "    return result_dict_avg, loss_avg\n",
        "\n",
        "\n",
        "# =================================================================================================================\n",
        "# some examples of the 'predictor' model wrapper to be fed into the above evaluation function (for PyG Data format)\n",
        "# =================================================================================================================\n",
        "class PygRandomPredictor:\n",
        "    def __init__(self):\n",
        "        # torch.manual_seed(0)\n",
        "        pass\n",
        "\n",
        "    def __call__(self, data):\n",
        "        pred_prob = torch.rand(len(data.y))\n",
        "        return pred_prob\n",
        "\n",
        "\n",
        "class PygModelPredictor:\n",
        "    def __init__(self, model, loss_fcn=torch.nn.CrossEntropyLoss()):\n",
        "        self.model = model\n",
        "        self.loss_fcn = loss_fcn\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    def __call__(self, data):\n",
        "        self.model.eval()\n",
        "        data = data.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            # custom the below line to adjust to your model's input format for forward pass\n",
        "            out = self.model(data.x, data.edge_index)\n",
        "            loss = self.loss_fcn(out, data.y.long())\n",
        "            pred_prob = torch.softmax(out, dim=1)[:, 1]\n",
        "        return pred_prob, loss.float()\n"
      ],
      "metadata": {
        "id": "pqjalkx1EmHc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Model"
      ],
      "metadata": {
        "id": "5fvRugR_E1QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GroupAddRev, SAGEConv, GINConv, GCNConv"
      ],
      "metadata": {
        "id": "iOPuMEcyvzPf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNorm(in_channels, elementwise_affine=True)\n",
        "        # self.conv = GCNConv(in_channels, out_channels)\n",
        "        channels = in_channels\n",
        "        self.conv = GINConv(\n",
        "                  Sequential(Linear(channels, channels),\n",
        "                             BatchNorm1d(channels), ReLU(),\n",
        "                             Linear(channels, channels),\n",
        "                             BatchNorm1d(channels), ReLU(),\n",
        "                             Linear(channels, channels), \n",
        "                             BatchNorm1d(channels), ReLU()))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.norm.reset_parameters()\n",
        "        self.conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, dropout_mask=None):\n",
        "        x = self.norm(x).relu()\n",
        "        if self.training and dropout_mask is not None:\n",
        "            x = x * dropout_mask\n",
        "        return self.conv(x, edge_index)"
      ],
      "metadata": {
        "id": "Vp2D8S_qvwh-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RevGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout, num_groups=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.lin1 = Linear(in_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, out_channels)\n",
        "        self.norm = LayerNorm(hidden_channels, elementwise_affine=True)\n",
        "\n",
        "        assert hidden_channels % num_groups == 0\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            conv = GNNBlock(\n",
        "                hidden_channels // num_groups,\n",
        "                hidden_channels // num_groups,\n",
        "            )\n",
        "            self.convs.append(GroupAddRev(conv, num_groups=num_groups))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin1.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "        self.norm.reset_parameters()\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.lin1(x)\n",
        "\n",
        "        # Generate a dropout mask which will be shared across GNN blocks:\n",
        "        mask = None\n",
        "        if self.training and self.dropout > 0:\n",
        "            mask = torch.zeros_like(x).bernoulli_(1 - self.dropout)\n",
        "            mask = mask.requires_grad_(False)\n",
        "            mask = mask / (1 - self.dropout)\n",
        "\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index, mask)\n",
        "        x = self.norm(x).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin2(x)"
      ],
      "metadata": {
        "id": "FGkeuO7c9jRd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. Activation "
      ],
      "metadata": {
        "id": "8rh6Ca1XE6ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation(act, negative_slope=0.2):\n",
        "    activations = nn.ModuleDict([\n",
        "        ['lrelu', nn.LeakyReLU(negative_slope)],\n",
        "        ['relu', nn.ReLU()],\n",
        "        ['elu', nn.ELU()],\n",
        "        ['none', nn.Identity()],\n",
        "    ])\n",
        "    return activations[act]"
      ],
      "metadata": {
        "id": "bzEVz1VwFjKK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. Load data"
      ],
      "metadata": {
        "id": "RRCYp7yuLxMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/Shareddrives/botnetdata/P2P'\n",
        "data_name = 'p2p' # 'chord', 'debru', 'kadem', 'leet', 'c2', 'p2p'\n",
        "shuffle = False\n",
        "\n",
        "# ========== load the dataset\n",
        "print('loading dataset...')\n",
        "\n",
        "train_ds = BotnetDataset(name=data_name, root=data_dir, split='train',\n",
        "                         in_memory=False, graph_format='pyg')\n",
        "val_ds = BotnetDataset(name=data_name, root=data_dir, split='val',\n",
        "                       in_memory=False, graph_format='pyg')\n",
        "test_ds = BotnetDataset(name=data_name, root=data_dir, split='test',\n",
        "                        in_memory=False, graph_format='pyg')\n"
      ],
      "metadata": {
        "id": "msBAy6CKLwl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c650fef-a6e1-4c69-9010-884d537bf0ef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8c4GEysKrMho"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "train_loader = GraphDataLoader(train_ds, batch_size=batch_size, shuffle=bool(shuffle), num_workers=0)\n"
      ],
      "metadata": {
        "id": "q23YTuSlmot5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VI. Train"
      ],
      "metadata": {
        "id": "PSY11P9WEX5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== some default parameters =============\n",
        "devid = 0\n",
        "seed = 0\n",
        "logmode = 'w'\n",
        "log_interval = 96\n",
        "\n",
        "dim_input_feature = 1\n",
        "dim_hidden_feature = 32\n",
        "dim_output_feature = 2\n",
        "act = 'relu' # 'none', 'lrelu', 'relu', 'elu'\n",
        "\n",
        "num_layers = 48\n",
        "num_classes = 2 \n",
        "\n",
        "dropout = 0.2\n",
        "bias = True\n",
        "\n",
        "lr = 0.005 # learning rate\n",
        "weight_decay = 5e-4\n",
        "epochs = 50\n",
        "save_dir = './saved_models'\n",
        "save_name = \"GIN_model.pt\"\n",
        "# ====================================================\n",
        "\n",
        "def train(model, train_loader, val_dataset, test_dataset, optimizer, scheduler=None):\n",
        "    device = next(model.parameters()).device\n",
        "    predictor = PygModelPredictor(model)\n",
        "\n",
        "    best_epoch = 0\n",
        "    max_f1_score = 0\n",
        "    start = time.time()\n",
        "    for ep in range(epochs):\n",
        "        loss_avg_train = 0\n",
        "        num_train_graph = 0\n",
        "        model.train()\n",
        "        for n, batch in enumerate(train_loader):\n",
        "            batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = model(batch.x, batch.edge_index)\n",
        "            loss = criterion(x, batch.y.long())\n",
        "\n",
        "            loss_avg_train += float(loss)\n",
        "            num_train_graph += batch.num_graphs\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if num_train_graph % log_interval == 0 or n == len(train_loader) - 1:\n",
        "                with torch.no_grad():\n",
        "                    # pred = x.argmax(dim=1)\n",
        "                    pred_prob = torch.softmax(x, dim=1)[:, 1]\n",
        "                    y = batch.y.long()\n",
        "                    result_dict = eval_metrics(y, pred_prob)\n",
        "                print(f'epoch: {ep + 1}, passed number of graphs: {num_train_graph}, '\n",
        "                        f'train running loss: {loss_avg_train / num_train_graph:.5f} (passed time: {time_since(start)})')\n",
        "                print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict.items()]))\n",
        "\n",
        "        result_dict_avg, loss_avg = eval_predictor(val_dataset, predictor)\n",
        "        print(f'Validation --- epoch: {ep + 1}, loss: {loss_avg:.5f}')\n",
        "        print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict_avg.items()]))\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(loss_avg)\n",
        "\n",
        "        if result_dict_avg['f1'] > max_f1_score:\n",
        "            save_name = f\"GIN_{ep}: {result_dict_avg['f1']}.pt\"\n",
        "            torch.save(model, os.path.join(save_dir, save_name))\n",
        "            print(f'Better model saved at {os.path.join(save_dir, save_name)}.')\n",
        "            best_epoch = ep\n",
        "            max_f1_score = result_dict_avg['f1']\n",
        "\n",
        "    best_model = torch.load(os.path.join(save_dir, save_name))\n",
        "    print('*' * 12 + f' best model obtained after epoch {best_epoch + 1}, '\n",
        "                       f'saved at {os.path.join(save_dir, save_name)} ' + '*' * 12)\n",
        "    \n",
        "    predictor = PygModelPredictor(best_model)\n",
        "\n",
        "    result_dict_avg, loss_avg = eval_predictor(test_dataset, predictor)\n",
        "    print(f'Testing --- loss: {loss_avg:.5f}')\n",
        "    print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict_avg.items()]))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # ========== random seeds and device\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    device = torch.device(f'cuda:{devid}') if devid > -1 else torch.device('cpu')\n",
        "\n",
        "    # ========== logging setup\n",
        "    log_name = os.path.splitext(save_name)[0]\n",
        "    # logger = logging_config(__name__, folder=save_dir, name=log_name, filemode=logmode)\n",
        "    # logger = logging_config(os.path.basename(__file__), folder=save_dir, name=log_name, filemode=logmode)\n",
        "\n",
        "    print('python ' + ' '.join(sys.argv))\n",
        "    print('-' * 30)\n",
        "    #logger.info(args)\n",
        "    print('-' * 30)\n",
        "    print(time.ctime())\n",
        "    print('-' * 30)\n",
        "\n",
        "    # ========== define the model, optimizer, and loss\n",
        "\n",
        "    model = RevGNN(in_channels=dim_input_feature,\n",
        "                   hidden_channels=32,\n",
        "                   out_channels=num_classes,\n",
        "                   num_layers=28,  # You can try 1000 layers for fun\n",
        "                   dropout=0.2,\n",
        "                   num_groups=2,\n",
        "    ).to(device)\n",
        "\n",
        "    print('model ' + '-' * 10)\n",
        "    print(repr(model))\n",
        "    model.to(device)\n",
        "    \n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=1)\n",
        "\n",
        "    # ========== train the model\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    train(model, train_loader, val_ds, test_ds, optimizer, scheduler)\n"
      ],
      "metadata": {
        "id": "3iFDupzCdBl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7316b87-b066-41a1-d88b-97313dac3b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-28276040-7352-44aa-9d48-4e0bf438edd1.json\n",
            "------------------------------\n",
            "------------------------------\n",
            "Mon Nov  7 02:44:20 2022\n",
            "------------------------------\n",
            "model ----------\n",
            "RevGNN(\n",
            "  (lin1): Linear(in_features=1, out_features=32, bias=True)\n",
            "  (lin2): Linear(in_features=32, out_features=2, bias=True)\n",
            "  (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  (convs): ModuleList(\n",
            "    (0): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (1): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (2): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (3): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (4): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (5): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (6): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (7): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (8): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (9): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (10): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (11): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (12): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (13): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (14): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (15): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (16): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (17): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (18): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (19): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (20): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (21): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (22): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (23): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (24): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (25): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (26): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "    (27): GroupAddRev(GNNBlock(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (conv): GINConv(nn=Sequential(\n",
            "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU()\n",
            "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU()\n",
            "      ))\n",
            "    ), num_groups=2)\n",
            "  )\n",
            ")\n",
            "Batch size: 2\n",
            "epoch: 1, passed number of graphs: 96, train running loss: 0.04890 (passed time: 3m 34s)\n",
            "          acc: 0.99369, fpr: 0.00002, fnr: 0.29016, rec: 0.70984, prc: 0.99888, f1: 0.82991, f1_macro: 0.91335\n",
            "epoch: 1, passed number of graphs: 192, train running loss: 0.02964 (passed time: 6m 1s)\n",
            "          acc: 0.99564, fpr: 0.00010, fnr: 0.19909, rec: 0.80091, prc: 0.99412, f1: 0.88712, f1_macro: 0.94245\n",
            "epoch: 1, passed number of graphs: 288, train running loss: 0.02255 (passed time: 8m 26s)\n",
            "          acc: 0.99743, fpr: 0.00007, fnr: 0.11781, rec: 0.88219, prc: 0.99613, f1: 0.93570, f1_macro: 0.96720\n",
            "epoch: 1, passed number of graphs: 384, train running loss: 0.01807 (passed time: 10m 53s)\n",
            "          acc: 0.99916, fpr: 0.00005, fnr: 0.03593, rec: 0.96407, prc: 0.99767, f1: 0.98058, f1_macro: 0.99008\n",
            "epoch: 1, passed number of graphs: 480, train running loss: 0.01503 (passed time: 13m 18s)\n",
            "          acc: 0.99940, fpr: 0.00000, fnr: 0.02744, rec: 0.97256, prc: 0.99983, f1: 0.98601, f1_macro: 0.99285\n",
            "epoch: 1, passed number of graphs: 576, train running loss: 0.01350 (passed time: 15m 46s)\n",
            "          acc: 0.99689, fpr: 0.00003, fnr: 0.14383, rec: 0.85617, prc: 0.99831, f1: 0.92179, f1_macro: 0.96010\n",
            "epoch: 1, passed number of graphs: 672, train running loss: 0.01198 (passed time: 18m 12s)\n",
            "          acc: 0.99871, fpr: 0.00028, fnr: 0.04730, rec: 0.95270, prc: 0.98709, f1: 0.96959, f1_macro: 0.98446\n",
            "epoch: 1, passed number of graphs: 768, train running loss: 0.01077 (passed time: 20m 36s)\n",
            "          acc: 0.99907, fpr: 0.00021, fnr: 0.03409, rec: 0.96591, prc: 0.99013, f1: 0.97787, f1_macro: 0.98870\n",
            "Validation --- epoch: 1, loss: 0.00702\n",
            "          f1_macro: 0.97412, rec: 0.97472, fpr: 0.00192, acc: 0.99758, f1: 0.94948, prc: 0.93399, fnr: 0.02528\n",
            "Better model saved at ./saved_models/GIN_0: 0.9494818315065521.pt.\n",
            "epoch: 2, passed number of graphs: 96, train running loss: 0.00730 (passed time: 24m 0s)\n",
            "          acc: 0.99931, fpr: 0.00000, fnr: 0.03142, rec: 0.96858, prc: 0.99984, f1: 0.98396, f1_macro: 0.99180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/RevGIN_24Layer_64Chan_053_12.zip /content/saved_models"
      ],
      "metadata": {
        "id": "hSEwE0hiltCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/RevGIN_24Layer_64Chan_053_12.zip\")"
      ],
      "metadata": {
        "id": "-IgoztfvlwKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ],
      "metadata": {
        "id": "VJTZlDLleS13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}